Supervised Learning 
1. What is Supervised Learning?

Supervised learning is a machine learning paradigm where a model learns a mapping from input features (X) to known output labels (y) using labeled training data.

Goal: Learn a function

f(X)â†’y

ğŸ“Œ Key idea: The correct answer is already known during training.

2. Types of Supervised Learning
1ï¸âƒ£ Classification

Predicts discrete / categorical outputs.

Examples:

Spam vs Not Spam

Disease vs No Disease

Fraud vs Non-fraud

Common algorithms:

Logistic Regression

K-Nearest Neighbors (KNN)

Naive Bayes

Support Vector Machine (SVM)

Decision Tree

Random Forest

Gradient Boosting (XGBoost, LightGBM)

Neural Networks

2ï¸âƒ£ Regression

Predicts continuous numerical values.

Examples:

House price prediction

Temperature forecasting

Salary prediction

Common algorithms:

Linear Regression

Polynomial Regression

Ridge / Lasso / ElasticNet

Decision Tree Regressor

Random Forest Regressor

Gradient Boosting Regressor

Neural Networks

3. Key Components of Supervised Learning
1ï¸âƒ£ Dataset

A labeled dataset consists of:

Features (X) â†’ independent variables

Target (y) â†’ dependent variable (label)

Example:

Size (sqft)	Bedrooms	Price
1200	2	300k
2ï¸âƒ£ Training Set, Validation Set, Test Set

Training set â†’ learns parameters

Validation set â†’ hyperparameter tuning

Test set â†’ final unbiased evaluation

Typical split:

70% Train

15% Validation

15% Test

3ï¸âƒ£ Model

A mathematical function with learnable parameters.

Example (Linear Regression):

y=wx+b
4ï¸âƒ£ Loss Function

Measures how wrong the modelâ€™s predictions are.

Task	Common Loss Functions
Regression	MSE, MAE, RMSE, Huber
Classification	Cross-Entropy, Log Loss, Hinge Loss
5ï¸âƒ£ Optimization Algorithm

Minimizes the loss function.

Examples:

Gradient Descent

Stochastic Gradient Descent (SGD)

Adam, RMSProp

4. Common Supervised Learning Algorithms (Interview Focus)
ğŸ”¹ Linear Regression

Assumes linear relationship

Minimizes Mean Squared Error

Sensitive to outliers

ğŸ“Œ Assumptions:

Linearity

Independence

Homoscedasticity

No multicollinearity

ğŸ”¹ Logistic Regression

Used for classification

Uses sigmoid function

Outputs probability (0â€“1)


Ïƒ(z)=(1+e)/(z-1)
	â€‹

ğŸ”¹ K-Nearest Neighbors (KNN)

Instance-based, lazy learner

No training phase

Sensitive to scale and choice of K

ğŸ”¹ Naive Bayes

Probabilistic classifier

Assumes feature independence

Fast and works well on text data

ğŸ”¹ Support Vector Machine (SVM)

Maximizes margin

Uses kernels for non-linear separation

Memory-intensive for large datasets

ğŸ”¹ Decision Tree

Rule-based model

Easy to interpret

Prone to overfitting

ğŸ”¹ Random Forest

Ensemble of decision trees

Reduces variance

Robust and widely used

ğŸ”¹ Gradient Boosting (XGBoost, LightGBM)

Sequential learning

Corrects previous errors

High performance but needs tuning

ğŸ”¹ Neural Networks

Multi-layer nonlinear models

Requires large data

Used in deep learning

5. Evaluation Metrics
ğŸ“Š Regression Metrics

Mean Squared Error (MSE)

Root Mean Squared Error (RMSE)

Mean Absolute Error (MAE)

RÂ² Score

ğŸ“Š Classification Metrics

Accuracy

Precision

Recall

F1-Score

ROC-AUC

Confusion Matrix

Use Precision when false positives are costly

Use Recall when false negatives are costly

6. Biasâ€“Variance Tradeoff

High Bias â†’ Underfitting

High Variance â†’ Overfitting

Problem	Solution
Underfitting	Add features, complex model
Overfitting	Regularization, more data, pruning
7. Regularization Techniques

Used to prevent overfitting.

L1 (Lasso) â†’ Feature selection

L2 (Ridge) â†’ Shrinks weights

ElasticNet â†’ Combination of L1 + L2

8. Feature Engineering (Very Important)

Handling missing values

Encoding categorical variables

Feature scaling (StandardScaler, MinMax)

Polynomial features

Feature selection

9. Data Leakage

Occurs when information from the test set leaks into training.

Example:

Scaling entire dataset before splitting


ğŸ“Œ Always fit preprocessors only on training data.


10. Real-World Supervised Learning Pipeline

Data collection

Data cleaning

Feature engineering

Train-test split

Model selection

Training

Hyperparameter tuning

Evaluation

Deployment

Monitoring
